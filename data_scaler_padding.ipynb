{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1+SE9jTgWFEck62V/89PU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andresdevml/dl_writing_quality/blob/main/data_scaler_padding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importamos librerias de interes**"
      ],
      "metadata": {
        "id": "kEP07UHPFVPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import keras\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import LSTM\n",
        "\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.layers import RepeatVector\n",
        "\n",
        "from keras.layers import TimeDistributed\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy import interpolate\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "JP_K7k0-FaYT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importamos la data**"
      ],
      "metadata": {
        "id": "2u4_thR-GXPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conectamos al drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definimos el directorio\n",
        "dir_data_kaggle='./drive/MyDrive/lwpwq/data'\n",
        "\n",
        "# extraemos la data\n",
        "\n",
        "# open a file, where you stored the pickled data\n",
        "file = open(dir_data_kaggle+'/train_data.pkl', 'rb')\n",
        "\n",
        "# dump information to that file\n",
        "train_data = pickle.load(file)\n",
        "\n",
        "# close the file\n",
        "file.close()\n",
        "\n",
        "# traemos los dataframes\n",
        "\n",
        "df_train_logs=pd.read_csv(filepath_or_buffer=dir_data_kaggle+'/train_logs.csv')\n",
        "\n",
        "df_train_scores=pd.read_csv(filepath_or_buffer=dir_data_kaggle+'/train_scores.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJkTy6vsGbKn",
        "outputId": "90634276-9586-4c91-8629-1d2588c1c75f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Escalamos la data**"
      ],
      "metadata": {
        "id": "hBVVAlflfU4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sabemos que dyn data tiene 8 columnas\n",
        "\n",
        "# ajustamos y guardamos los scalers\n",
        "\n",
        "list_scaler=[]\n",
        "\n",
        "for k in range(8):\n",
        "\n",
        "  list_seq=[]\n",
        "  for id in train_data:\n",
        "    seq=list(train_data[id]['dyn_data'][:,k])\n",
        "    list_seq=list_seq+seq\n",
        "\n",
        "  list_scaler.append(\n",
        "                      MinMaxScaler(feature_range=(0, 5))\n",
        "                      .fit(\n",
        "                               np.array(list_seq).reshape(-1,1)\n",
        "                                                                  )\n",
        "                                                                               )"
      ],
      "metadata": {
        "id": "6mZk77qEfYJZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aplicamos el escaler para cada secuencia en cada columna de dyn data\n",
        "\n",
        "for id in train_data:\n",
        "\n",
        "  dyn_data=train_data[id]['dyn_data']\n",
        "\n",
        "  train_data[id]['dyn_data']=np.hstack(tup=tuple(\n",
        "\n",
        "                  [ list_scaler[k].transform(dyn_data[:,k].reshape(-1,1)) for k in range(8) ]\n",
        "\n",
        "                                              )\n",
        "                                                  )"
      ],
      "metadata": {
        "id": "zz6Nm0Vm-fTe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Aplicamos Padding sobre la data**"
      ],
      "metadata": {
        "id": "g0ZWHvj8L0LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# determianmos la longitud maxima de las secuencias\n",
        "\n",
        "max_len=int(df_train_logs['event_id'].max()*1.20)\n",
        "\n",
        "print(max_len)\n"
      ],
      "metadata": {
        "id": "ZVbj7sp8L33w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e2aee2-bfda-4b19-e691-825780dbe003"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generamos la lista para cada columna\n",
        "\n",
        "list_data=[]\n",
        "\n",
        "for column in range(8):\n",
        "\n",
        "  list_seq=[]\n",
        "\n",
        "  for id in train_data:\n",
        "    list_seq.append(train_data[id]['dyn_data'][:,column])\n",
        "\n",
        "  list_data.append(list_seq)"
      ],
      "metadata": {
        "id": "TYCO2VYUTC6S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aplicamos padding a cada columna y juntamos las columns\n",
        "pad_list_data=[tf.keras.utils.pad_sequences(\n",
        "                                            sequences=column,\n",
        "                                              maxlen=max_len,\n",
        "                                              dtype='float64',\n",
        "                                              padding='post',\n",
        "                                              truncating='pre',\n",
        "                                              value=-1\n",
        "\n",
        "                                                    ) for column in list_data\n",
        "                                                              ]\n",
        "\n",
        "\n",
        "pad_train_data=np.hstack(tup=tuple(pad_list_data)).reshape(len(train_data),-1,8)"
      ],
      "metadata": {
        "id": "30L8-dKAqCcy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Colocamos la data estatica en forma de array**"
      ],
      "metadata": {
        "id": "5jgTv12c--ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stat_train_data=np.vstack(\n",
        "\n",
        "\n",
        "  tup=tuple([train_data[id]['stat_data'] for id in train_data]))\n"
      ],
      "metadata": {
        "id": "vEh2d7yU-9ON"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFxBwb6EpGiE",
        "outputId": "6e652b60-5cdc-44c5-df1e-9b186dedef0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.21394059, 0.18525641, 0.20676692])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exportamos la data**"
      ],
      "metadata": {
        "id": "sK6I4m9guepi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Open a file and use dump()\n",
        "with open( dir_data_kaggle + '/pad_train_data.pkl', 'wb' ) as file:\n",
        "\n",
        "    # A new file will be created\n",
        "    pickle.dump(pad_train_data, file)\n",
        "\n",
        "\n",
        "# Open a file and use dump()\n",
        "with open( dir_data_kaggle + '/stat_train_data.pkl', 'wb' ) as file:\n",
        "\n",
        "    # A new file will be created\n",
        "    pickle.dump(stat_train_data, file)"
      ],
      "metadata": {
        "id": "JotHMbcWz10I"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}