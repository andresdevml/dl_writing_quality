{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMW1Itfpq6T74NGy9CEvTY0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andresdevml/dl_writing_quality/blob/main/data_wraling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importamos librerias de interes**"
      ],
      "metadata": {
        "id": "A83ZM-GHImsk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGaCrRkxIga6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import keras\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy import interpolate\n",
        "\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Funciones de trabajo**"
      ],
      "metadata": {
        "id": "QCNpWNw7Kj2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las funciones de transformacion\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "def fun_action_time(df_data):\n",
        "\n",
        "  std_scaler=StandardScaler()\n",
        "\n",
        "  action_time=df_data['action_time']\n",
        "\n",
        "  action_time=std_scaler.fit_transform(\n",
        "                                    df_data['action_time'].values.reshape(-1,1))\n",
        "\n",
        "  return action_time\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "def fun_delay_time(df_data):\n",
        "\n",
        "  std_scaler=StandardScaler()\n",
        "\n",
        "  up_time_displaced=pd.concat(\n",
        "                              objs=[  pd.Series([0]) ,   df_data['up_time']  ],\n",
        "                              ignore_index=True\n",
        "                                                            )\n",
        "\n",
        "  delay_time=std_scaler.fit_transform(\n",
        "                                  (df_data['down_time'].values-up_time_displaced\n",
        "                                    .iloc[0:-1]\n",
        "                                    .values)\n",
        "                                    .reshape(-1,1)\n",
        "                                                            )\n",
        "  return delay_time\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "def fun_activity(df_data):\n",
        "\n",
        "  vocab_cut=['Non', 'Inp', 'Rem', 'Rep', 'Mov', 'Pas']\n",
        "\n",
        "  map_dic_activity={'Inp': 0.22, 'Rem': 2.38, 'Non': 4.86, 'Rep': 12.41,\n",
        "                    'Pas': 21.96, 'Mov': 34.07}\n",
        "\n",
        "  activity_cut=df_data['activity'].apply(lambda x: x[:3])\n",
        "\n",
        "\n",
        "  cat_activity=np.array(\n",
        "                activity_cut.apply(lambda x: map_dic_activity[x])).reshape(-1,1)\n",
        "\n",
        "  return cat_activity\n",
        "\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "# funcion auxiliar 1\n",
        "def mod_act(str_arr):\n",
        "  if str_arr=='NoChange':\n",
        "    return 0\n",
        "  else:\n",
        "    if ' => ' in str_arr:\n",
        "      str_split=str_arr.split(' => ', maxsplit=1)\n",
        "      return len(str_split[0])+len(str_split[1])\n",
        "    else:\n",
        "      return len(str_arr)\n",
        "\n",
        "# funcion auxiliar 2\n",
        "def cut_act(len_act):\n",
        "  if len_act>=2:\n",
        "    return 2\n",
        "  else:\n",
        "    return len_act\n",
        "\n",
        "def fun_activity_mod(df_data):\n",
        "\n",
        "  map_dic_mod_activity={1: 0.08853, 0: 2.5686, 2: 9.4840}\n",
        "\n",
        "  activity_mod=df_data['text_change'].apply(mod_act).apply(cut_act)\n",
        "\n",
        "  cat_activity_mod=np.array(activity_mod.apply(\n",
        "                              lambda x: map_dic_mod_activity[x])).reshape(-1,1)\n",
        "\n",
        "\n",
        "  return cat_activity_mod\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "def fun_event(df_data):\n",
        "\n",
        "  vocab=['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick',\n",
        "         'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock']\n",
        "\n",
        "  layer = tf.keras.layers.StringLookup(vocabulary=vocab,\n",
        "                                      num_oov_indices=1,\n",
        "                                     output_mode='int')\n",
        "\n",
        "  map_dic_event={1: 0.45, 2: 2.40, 3: 4.56, 4: 7.78, 5: 12.018, 6: 16.54,\n",
        "                 7: 21.43, 8: 26.38, 0: 31.34, 9: 36.53, 10: 42.13,\n",
        "                 11: 47.83, 12: 53.75,\n",
        "                 13: 60.25}\n",
        "\n",
        "\n",
        "  event=pd.Series(layer(df_data['up_event'])).apply(\n",
        "                              lambda x: map_dic_event[x]).values.reshape(-1,1)\n",
        "\n",
        "  return event\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "def fun_cursor_position(df_data):\n",
        "\n",
        "\n",
        "  std_scaler=StandardScaler()\n",
        "\n",
        "  cursor_position=df_data['cursor_position'].values\n",
        "\n",
        "  x_cursor_position=p.arange(len(cursor_position)).reshape(-1,1)\n",
        "\n",
        "  # definimos el svr\n",
        "\n",
        "  regr_cp = make_pipeline(StandardScaler(), svm.SVR(kernel='linear',C=100\n",
        "                                                            , epsilon=10e-21))\n",
        "\n",
        "  regr_cp.fit(np.arangex_cursor_position,cursor_position.reshape(-1,1))\n",
        "\n",
        "  cursor_position_lin=regr_cp.predict(x_cursor_position)\n",
        "\n",
        "  cursor_position_flat=(\n",
        "                       (-1)*(cursor_position-cursor_position_lin)\n",
        "                                                                 ).reshape(-1,1)\n",
        "\n",
        "  # aqui estandarizamos\n",
        "\n",
        "  cursor_position_flat=std_scaler.fit_transform(cursor_position_flat)\n",
        "\n",
        "  return cursor_position_flat\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "def fun_word_count(df_data):\n",
        "\n",
        "  std_scaler=StandardScaler()\n",
        "\n",
        "  word_count=df_data['word_count'].values\n",
        "\n",
        "  x_word_count=p.arange(len(word_count)).reshape(-1,1)\n",
        "\n",
        "  regr_wc = make_pipeline(StandardScaler(), svm.SVR(kernel='linear',C=100,\n",
        "                                                    epsilon=10e-21))\n",
        "\n",
        "  regr_wc.fit(\n",
        "    x_word_count,\n",
        "    word_count.reshape(-1,1))\n",
        "\n",
        "  word_count_lin=regr_wc.predict(x_word_count)\n",
        "\n",
        "  word_count_flat=((-1)*(word_count-word_count_lin)).reshape(-1,1)\n",
        "  word_count_flat=std_scaler.fit_transform(word_count_flat) # aqui estandarizamos\n",
        "\n",
        "  # hacemos la interpolacion\n",
        "\n",
        "  index_interp=np.floor(np.linspace(x_word_count[0], x_word_count[-1] ,\n",
        "                                    int(len(x_word_count)*0.0075))).astype(int)\n",
        "\n",
        "  x_intp=x_word_count[index_interp]\n",
        "\n",
        "  y=np.squeeze(word_count_flat[index_interp])\n",
        "\n",
        "  f = interpolate.interp1d(x_intp, y,'nearest-up')\n",
        "\n",
        "  word_count_flat_intp=f(x_word_count).reshape(-1,1)\n",
        "\n",
        "  return word_count_flat_intp\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "\n",
        "# funcion auxiliar\n",
        "\n",
        "def text_change_format(pd_serie):\n",
        "\n",
        "  aux_serie=pd_serie.copy(deep=True)\n",
        "  for row,value in enumerate(aux_serie):\n",
        "    if ' => ' in value:\n",
        "      aux_serie.iloc[row]='=>'\n",
        "  return aux_serie\n",
        "\n",
        "\n",
        "\n",
        "def fun_change_text(df_data):\n",
        "\n",
        "  vocab_tc=['q' ' ' 'NoChange' '.' ',' '\\n' \"'\" '\"' '-' '=>' '?' ';']\n",
        "\n",
        "  layer_tc= tf.keras.layers.StringLookup(vocabulary=vocab_tc,\n",
        "                                            num_oov_indices=1,\n",
        "                                                  output_mode='int')\n",
        "\n",
        "  map_dic_tc={1: 0.31, 2: 2.12, 3: 4.60, 4: 9.42, 5: 14.45, 6: 20.16, 0: 26.61,\n",
        "              7: 33.07, 8: 40.57, 9: 48.10, 10: 55.65, 11: 63.43, 12: 71.40}\n",
        "\n",
        "\n",
        "\n",
        "  tc_transf=text_change_format(df_data['text_change'])\n",
        "\n",
        "\n",
        "  tc_data=pd.Series(layer(tc_transf)).apply(\n",
        "                                  lambda x: map_dic_tc[x]).values.reshape(-1,1)\n",
        "\n",
        "  return tc_data\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "def fun_len_time(df_data):\n",
        "\n",
        "  max_len_time=8313707\n",
        "\n",
        "  max_ts_len_time=np.maximum(df_data['up_time'].max(),\n",
        "                             df_data['down_time'].max())\n",
        "\n",
        "  return max_ts_len_time/max_len_time\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uxlrByd5KoMC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_yusS1GGFDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vOzvSzVcGH2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importamos la data**"
      ],
      "metadata": {
        "id": "AYOb7PbUItyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conectamos al drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definimos el directorio\n",
        "dir_data_kaggle='./drive/MyDrive/lwpwq/data'\n",
        "\n",
        "\n",
        "# extraemos la data\n",
        "\n",
        "df_train_logs=pd.read_csv(filepath_or_buffer=dir_data_kaggle+'/train_logs.csv')\n",
        "\n",
        "df_train_scores=pd.read_csv(filepath_or_buffer=dir_data_kaggle+'/train_scores.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKh_iMwHIvp_",
        "outputId": "24c15723-f9f1-45d9-c4c0-a614ae6ce5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# observemos las columnas presentes\n",
        "\n",
        "df_train_logs.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abNDB-KBIrN-",
        "outputId": "41e681ed-4801-489a-c5dc-3bbb190ee253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'event_id', 'down_time', 'up_time', 'action_time', 'activity',\n",
              "       'down_event', 'up_event', 'text_change', 'cursor_position',\n",
              "       'word_count'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Probamos con una instancia**"
      ],
      "metadata": {
        "id": "3SuyxDqk_M4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# extraemos los ids\n",
        "\n",
        "list_id=list(df_train_scores['id'].unique())\n",
        "\n",
        "# inicializamos el diccionario\n",
        "\n",
        "dic_data_train={ }\n",
        "\n",
        "\n",
        "# iniciamos el proceso\n",
        "\n",
        "\n",
        "for id in list_id:\n",
        "\n",
        "  df_data=df_train_logs[df_train_logs['id']==id]\n",
        "\n"
      ],
      "metadata": {
        "id": "ReVgo-wKJJL8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}