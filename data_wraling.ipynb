{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ176yUsuHUYIriP8PYY4i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andresdevml/dl_writing_quality/blob/main/data_wraling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importamos librerias de interes**"
      ],
      "metadata": {
        "id": "A83ZM-GHImsk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGaCrRkxIga6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import keras\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy import interpolate\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Funciones de trabajo**"
      ],
      "metadata": {
        "id": "QCNpWNw7Kj2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las funciones de transformacion\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "def fun_action_time(df_data):\n",
        "\n",
        "  action_time=df_data['action_time']\n",
        "\n",
        "  action_time=df_data['action_time'].values.reshape(-1,1)\n",
        "\n",
        "  return action_time\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "def fun_delay_time(df_data):\n",
        "\n",
        "  up_time_displaced=pd.concat(\n",
        "                              objs=[  pd.Series([0]) ,   df_data['up_time']  ],\n",
        "                              ignore_index=True\n",
        "                                                            )\n",
        "\n",
        "  delay_time=(df_data['down_time'].values -\n",
        "                              up_time_displaced.iloc[0:-1].values\n",
        "                                                                ).reshape(-1,1)\n",
        "\n",
        "  return delay_time\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "def fun_activity(df_data):\n",
        "\n",
        "  vocab_cut=['Non', 'Inp', 'Rem', 'Rep', 'Mov', 'Pas']\n",
        "\n",
        "  map_dic_activity={'Inp': 0.22, 'Rem': 2.38, 'Non': 4.86, 'Rep': 12.41,\n",
        "                    'Pas': 21.96, 'Mov': 34.07}\n",
        "\n",
        "  activity_cut=df_data['activity'].apply(lambda x: x[:3])\n",
        "\n",
        "\n",
        "  cat_activity=np.array(\n",
        "                activity_cut.apply(lambda x: map_dic_activity[x])).reshape(-1,1)\n",
        "\n",
        "  return cat_activity\n",
        "\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "# funcion auxiliar 1\n",
        "def mod_act(str_arr):\n",
        "  if str_arr=='NoChange':\n",
        "    return 0\n",
        "  else:\n",
        "    if ' => ' in str_arr:\n",
        "      str_split=str_arr.split(' => ', maxsplit=1)\n",
        "      return len(str_split[0])+len(str_split[1])\n",
        "    else:\n",
        "      return len(str_arr)\n",
        "\n",
        "# funcion auxiliar 2\n",
        "def cut_act(len_act):\n",
        "  if len_act>=2:\n",
        "    return 2\n",
        "  else:\n",
        "    return len_act\n",
        "\n",
        "def fun_activity_mod(df_data):\n",
        "\n",
        "  map_dic_mod_activity={1: 0.08853, 0: 2.5686, 2: 9.4840}\n",
        "\n",
        "  activity_mod=df_data['text_change'].apply(mod_act).apply(cut_act)\n",
        "\n",
        "  cat_activity_mod=np.array(activity_mod.apply(\n",
        "                              lambda x: map_dic_mod_activity[x])).reshape(-1,1)\n",
        "\n",
        "\n",
        "  return cat_activity_mod\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "def fun_event(df_data):\n",
        "\n",
        "  vocab=['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick',\n",
        "         'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock']\n",
        "\n",
        "  layer = tf.keras.layers.StringLookup(vocabulary=vocab,\n",
        "                                      num_oov_indices=1,\n",
        "                                     output_mode='int')\n",
        "\n",
        "  map_dic_event={1: 0.45, 2: 2.40, 3: 4.56, 4: 7.78, 5: 12.018, 6: 16.54,\n",
        "                 7: 21.43, 8: 26.38, 0: 31.34, 9: 36.53, 10: 42.13,\n",
        "                 11: 47.83, 12: 53.75,\n",
        "                 13: 60.25}\n",
        "\n",
        "\n",
        "  event=pd.Series(layer(df_data['up_event'])).apply(\n",
        "                              lambda x: map_dic_event[x]).values.reshape(-1,1)\n",
        "\n",
        "  return event\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "def fun_cursor_position(df_data):\n",
        "\n",
        "  cursor_position=df_data['cursor_position'].values\n",
        "\n",
        "  x_cursor_position=np.arange(len(cursor_position)).reshape(-1,1)\n",
        "\n",
        "  # definimos el svr\n",
        "\n",
        "  regr_cp = make_pipeline(StandardScaler(), svm.SVR(kernel='linear',C=100\n",
        "                                                            , epsilon=10e-21))\n",
        "\n",
        "  regr_cp.fit(x_cursor_position,\n",
        "              cursor_position)\n",
        "\n",
        "  cursor_position_lin=regr_cp.predict(x_cursor_position)\n",
        "\n",
        "  cursor_position_flat=(cursor_position_lin-cursor_position).reshape(-1,1)\n",
        "\n",
        "  return cursor_position_flat\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "def fun_word_count(df_data):\n",
        "\n",
        "  std_scaler=StandardScaler()\n",
        "\n",
        "  word_count=df_data['word_count'].values\n",
        "\n",
        "  x_word_count=np.arange(len(word_count)).reshape(-1,1)\n",
        "\n",
        "  regr_wc = make_pipeline(StandardScaler(), svm.SVR(kernel='linear',C=100,\n",
        "                                                    epsilon=10e-21))\n",
        "\n",
        "  regr_wc.fit(\n",
        "    x_word_count,\n",
        "    word_count)\n",
        "\n",
        "  word_count_lin=regr_wc.predict(x_word_count)\n",
        "\n",
        "  word_count_flat=(word_count_lin-word_count).reshape(-1,1)\n",
        "\n",
        "  # hacemos la interpolacion\n",
        "\n",
        "  index_interp=np.floor(np.linspace(start=x_word_count[0],\n",
        "                                      stop=x_word_count[-1] ,\n",
        "                                          num=20)).astype(int)\n",
        "\n",
        "  x_intp=np.squeeze(x_word_count[index_interp])\n",
        "\n",
        "  y=np.squeeze(word_count_flat[index_interp])\n",
        "\n",
        "  f = interpolate.interp1d(x_intp, y,'nearest-up')\n",
        "\n",
        "  word_count_flat_intp=f(x_word_count).reshape(-1,1)\n",
        "\n",
        "  return word_count_flat_intp\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "\n",
        "# funcion auxiliar\n",
        "\n",
        "def text_change_format(row):\n",
        "\n",
        "  if ' => ' in row:\n",
        "    return '=>'\n",
        "  else:\n",
        "    return row\n",
        "\n",
        "def fun_change_text(df_data):\n",
        "\n",
        "  vocab_tc=['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '=>', '?',';']\n",
        "\n",
        "  layer_tc= tf.keras.layers.StringLookup(vocabulary=vocab_tc,\n",
        "                                            num_oov_indices=1,\n",
        "                                                  output_mode='int')\n",
        "\n",
        "  map_dic_tc={1: 0.31, 2: 2.12, 3: 4.60, 4: 9.42, 5: 14.45, 6: 20.16, 0: 26.61,\n",
        "              7: 33.07, 8: 40.57, 9: 48.10, 10: 55.65, 11: 63.43, 12: 71.40}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  tc_data=pd.Series(\n",
        "          layer_tc(\n",
        "              df_data['text_change'].apply(text_change_format)\n",
        "                                        )\n",
        "                                            ).apply(lambda x:\n",
        "                                             map_dic_tc[x]).values.reshape(-1,1)\n",
        "\n",
        "  return tc_data\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "def fun_len_time(df_data):\n",
        "\n",
        "  max_len_time=8313707\n",
        "\n",
        "  max_ts_len_time=np.maximum(df_data['up_time'].max(),\n",
        "                             df_data['down_time'].max())\n",
        "\n",
        "  return max_ts_len_time/max_len_time\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "def fun_len_word(df_data):\n",
        "\n",
        "  max_len_word=1326\n",
        "\n",
        "  max_ts_len_word=df_data['word_count'].max()\n",
        "\n",
        "  return max_ts_len_word/max_len_word\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "def fun_len_cursor_position(df_data):\n",
        "\n",
        "  max_cursor_position=7802\n",
        "\n",
        "  max_ts_cursor_position=df_data['cursor_position'].max()\n",
        "\n",
        "  return max_ts_cursor_position/max_cursor_position"
      ],
      "metadata": {
        "id": "uxlrByd5KoMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importamos la data**"
      ],
      "metadata": {
        "id": "AYOb7PbUItyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conectamos al drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definimos el directorio\n",
        "dir_data_kaggle='./drive/MyDrive/lwpwq/data'\n",
        "\n",
        "\n",
        "# extraemos la data\n",
        "\n",
        "df_train_logs=pd.read_csv(filepath_or_buffer=dir_data_kaggle+'/train_logs.csv')\n",
        "\n",
        "df_train_scores=pd.read_csv(filepath_or_buffer=dir_data_kaggle+'/train_scores.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKh_iMwHIvp_",
        "outputId": "d257431c-eb9e-4c73-841c-6cf367b10cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# observemos las columnas presentes\n",
        "\n",
        "df_train_logs.columns"
      ],
      "metadata": {
        "id": "abNDB-KBIrN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84fdc00e-9bb8-4593-bbb8-19920ab01641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'event_id', 'down_time', 'up_time', 'action_time', 'activity',\n",
              "       'down_event', 'up_event', 'text_change', 'cursor_position',\n",
              "       'word_count'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformamos toda la data**"
      ],
      "metadata": {
        "id": "lU18jCVczJZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_id=df_train_logs['id'].unique()\n",
        "\n",
        "train_data={}\n",
        "\n",
        "for id in list_id:\n",
        "  df_data=df_train_logs[df_train_logs['id']==id]\n",
        "\n",
        "  # concatenamos la data\n",
        "\n",
        "  dyn_data=np.hstack(\n",
        "                      tup=(\n",
        "\n",
        "    fun_action_time(df_data),\n",
        "    fun_delay_time(df_data),\n",
        "    fun_activity(df_data),\n",
        "    fun_activity_mod(df_data),\n",
        "    fun_event(df_data),\n",
        "    fun_cursor_position(df_data),\n",
        "    fun_word_count(df_data),\n",
        "    fun_change_text(df_data)\n",
        "                                    )\n",
        "                                                  )\n",
        "\n",
        "  min_max_scaler = MinMaxScaler(feature_range=(0, 5))\n",
        "\n",
        "  dyn_data=min_max_scaler.fit_transform(dyn_data)\n",
        "\n",
        "  stat_data=np.hstack(\n",
        "\n",
        "                          tup=(\n",
        "    fun_len_time(df_data),\n",
        "    fun_len_word(df_data),\n",
        "    fun_len_cursor_position(df_data)\n",
        "                                        )\n",
        "\n",
        "                              )\n",
        "\n",
        "  target=df_train_scores[df_train_scores['id']==id]['score'].values\n",
        "\n",
        "  train_data[id]={\n",
        "\n",
        "        'dyn_data':dyn_data,\n",
        "        'stat_data':stat_data,\n",
        "        'target':target\n",
        "                                }"
      ],
      "metadata": {
        "id": "csRwh_SxzLq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exportamos la data**"
      ],
      "metadata": {
        "id": "c97NYG0zCT8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Open a file and use dump()\n",
        "with open( dir_data_kaggle + '/train_data.pkl', 'wb' ) as file:\n",
        "\n",
        "    # A new file will be created\n",
        "    pickle.dump(train_data, file)\n"
      ],
      "metadata": {
        "id": "gPhahU0NCTSE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}